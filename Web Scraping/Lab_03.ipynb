{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#<img style=\"float: left; padding-right: 10px; width: 45px;\" src=\"https://www.eyeofriyadh.com/includes/image.php?image=/directory/images/2018/04/273d4696fbb5d.png&width=50&height=50\"> Web Scrapping (Lab 03)\n",
        "\n",
        "**Taibah University**<br/>\n",
        "**$3^{rd}$ Term 2023**<br/>\n",
        "**Instructors**: Prof. Dr. Mohammed Al-Sarem\n",
        "<hr style='height:2px'>"
      ],
      "metadata": {
        "id": "CU4AsXHykkZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Table of Contents:\n",
        "* Prerequisites\n",
        "* Introduction to web scraping: [Selenuim](https://selenium-python.readthedocs.io/getting-started.html) vs [Beautiful Soap](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) package\n",
        " * Setting the environment\n",
        " * Parse the HTML with __BeautifulSoup__\n",
        "* Concatenate and merge two or more dataframes\n",
        "* ٍSave data frame as __*.csv__ file"
      ],
      "metadata": {
        "id": "gu4MzMKkk5DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "\n",
        "from urllib.request import Request, urlopen\n",
        "#import urllib\n",
        "import urllib.error\n",
        "\n",
        "\n",
        "import time\n",
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# Setting up 'requests' to make HTTPS requests properly takes some extra steps... we'll skip them for now.\n",
        "requests.packages.urllib3.disable_warnings()"
      ],
      "metadata": {
        "id": "a1lZ5_bFn4yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology\n",
        "Here’s step by step outline of this project:\n",
        "\n",
        "* Download the google scholar webpage of related topic using requests\n",
        "* Parse the HTML code source code using beautiful soup\n",
        "* Extract Title of the paper , Number of citation , Author of the paper , Year of Publication , Place of Publication from page\n",
        "* Compile the data and create a CSV file using pandas\n",
        "\n",
        "### Download the __Google scholar citation__ webpage using requests\n",
        "To begin , we’ll use the requests Python library to download the web page. We can use ___requests.get___ to download a page . Here we also need to define __headers__ in this function because google scholar webpage required login.\n"
      ],
      "metadata": {
        "id": "TcAFQRpak1w3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.1:** Write function named _get_link_to_profile( )_ that takes a path to data source ([Pandas Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)) that was genereted in __LAB 01 (Web Scraping)__ and returns a sliced pd.Series which contains active link to researchers' google citation profiles."
      ],
      "metadata": {
        "id": "7wL-rA50oGAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wrangle(file_path):\n",
        "  \"\"\"Creates CSV file with all authors' information.\n",
        "\n",
        "    CSV file name will include today's date, e.g. `'2023-03-28__authors_list.csv`'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name : suggested name\n",
        "        csv file name where you saved the data obtained at the previous lab.\n",
        "    Returns\n",
        "    -------\n",
        "    mask_lnk: pd.Series\n",
        "  \"\"\"\n",
        "  # Read CSV file into DataFrame\n",
        "  df = pd.read_csv(file_path)\n",
        "  # Subset to google citation link of researchers' profiles\n",
        "  mask_lnk = df [\"link\"]\n",
        "\n",
        "  return pd.Series(mask_lnk)"
      ],
      "metadata": {
        "id": "QjzxAPnsn_Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.2:** Use your `wrangle` function to create a DataFrame `df` from the CSV file `./content/2023-03-23_authors_list.csv`.\n",
        "__Attention:__\n",
        ">  **Note!⚠️**:You have to pass the right name of your dataset. Note that, the name of *.csv has the following convension: **date in (%Y-%m-%d) format followed by ___authors_list.csv__."
      ],
      "metadata": {
        "id": "HX39z1z3oVTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = wrangle(\"/content/2023-03-23_authors_list.csv\")\n",
        "print(\"df shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbH5R_N_oXdP",
        "outputId": "a47456b7-8745-4640-9406-dea9ba617635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df shape: (404,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    https://scholar.google.com//citations?hl=en&us...\n",
              "1    https://scholar.google.com/citations?hl=en&use...\n",
              "2    https://scholar.google.com/citations?hl=en&use...\n",
              "3    https://scholar.google.com//citations?hl=en&us...\n",
              "4    https://scholar.google.com//citations?hl=en&us...\n",
              "Name: link, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, your DataFrame `df` should have no more than 404 observations."
      ],
      "metadata": {
        "id": "tXlCzI7roc6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 3.3__: Set your user agent and assign it to avarible name _user_agent_. Then create a dictinary _headers_ and store your user agent."
      ],
      "metadata": {
        "id": "7-lClFBxol09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'\n",
        "headers = {'User-Agent': user_agent}"
      ],
      "metadata": {
        "id": "gywGMJtHorp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 3.4:__ create a function called __metric()__ that parse the html content and return a data frame with the follwing information:\n",
        "  * _total citation_\n",
        "  * _citation since last 5 years_\n",
        "  * _h-index_\n",
        "  * _h-index since last 5 year_\n",
        "  * _i10h-index_\n",
        "  * and, _i10h-index since last 5year_.\n"
      ],
      "metadata": {
        "id": "oZfESgDLu6E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metric(df):\n",
        "\n",
        "\n",
        "  info=[]\n",
        "  for record in df:\n",
        "    col_data=[]\n",
        "    time.sleep(0.70)\n",
        "    url= record\n",
        "    reqest = Request(url,headers=headers)\n",
        "    page = urlopen(reqest)\n",
        "    soup = BeautifulSoup(page, \"lxml\")\n",
        "\n",
        "    print(f'The requested page: {url} is being process')\n",
        "    try:\n",
        "      table= soup.find('table',{'id':'gsc_rsb_st'})\n",
        "      table_body = table.find('tbody')\n",
        "      rows = table_body.find_all('tr')\n",
        "      for row in rows:\n",
        "        cols = row.find_all('td',{'class':'gsc_rsb_std'})\n",
        "        cols = [ele.text.strip() for ele in cols]\n",
        "        col_data.append([ele for ele in cols if ele])\n",
        "      info.append({\n",
        "                      'auth_prf': url,\n",
        "                      'all_citation': col_data[0][0],\n",
        "                      'since_2018': col_data[0][1],\n",
        "                      'h-index_all': col_data[1][0],\n",
        "                      'h-index_2018':col_data[1][1],\n",
        "                      'i10_h_index_all':col_data[2][0],\n",
        "                      'i10_h_index_2018':col_data[2][1]\n",
        "                })\n",
        "    except Exception as e:\n",
        "       print('the following error is raised '+ str(e))\n",
        "       info.append({\n",
        "                  'auth_prf': url,\n",
        "                  'all_citation': 0,\n",
        "                  'since_2018': 0,\n",
        "                  'h-index_all': 0,\n",
        "                  'h-index_2018':0,\n",
        "                  'i10_h_index_all':0,\n",
        "                  'i10_h_index_2018':0\n",
        "                  })\n",
        "\n",
        "\n",
        "  return info\n",
        "\n",
        "record= metric(df)\n",
        "dataframe= pd.DataFrame(record)\n",
        "dataframe.head()"
      ],
      "metadata": {
        "id": "zk7Vw3qU2pRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 3.5:__ It is time to save the collected data as _*.csv file_. Create a function called __save_as_csv()__ which takes as a parameter: __name of file__, __the directory__, and dataframe that you generated before."
      ],
      "metadata": {
        "id": "UueCW5k_Myza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_as_csv(file_name: str, directory= \".\"):\n",
        "  \"\"\"Creates CSV file with all authors' information.\n",
        "    CSV file name will include today's date, e.g. `'authors_h-index.csv`'.\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name : suggested name\n",
        "        Observations with group assignment.\n",
        "    directory : str, default='.'\n",
        "        Location for saved CSV file.\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "  # Create filename with date\n",
        "  date_string = pd.Timestamp.now().strftime(format= \"%Y-%m-%d\")\n",
        "  final_filename = directory + \"/\" + date_string +'_' + file_name+ '.csv'\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "  dataframe.to_csv(final_filename)\n",
        "\n",
        "save_as_csv('authors_h-index')"
      ],
      "metadata": {
        "id": "-1H90KeIM9N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "import time\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "out = display(progress(0, 100), display_id=True)\n",
        "for ii in range(101):\n",
        "    time.sleep(0.02)\n",
        "    out.update(progress(ii, 100))"
      ],
      "metadata": {
        "id": "OmrinZMTpKlA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}