{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#<img style=\"float: left; padding-right: 10px; width: 45px;\" src=\"https://www.eyeofriyadh.com/includes/image.php?image=/directory/images/2018/04/273d4696fbb5d.png&width=50&height=50\"> Web Scrapping (Lab 02)\n",
        "\n",
        "**Taibah University**<br/>\n",
        "**$3^{rd}$ Term 2023**<br/>\n",
        "**Instructors**: Prof. Dr. Mohammed Al-Sarem\n",
        "<hr style='height:2px'>"
      ],
      "metadata": {
        "id": "w6dc8eN-hR-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we're done today, you will approach messy real-world data with confidence that you can get it into a format that you can manipulate.\n",
        "\n",
        "Specifically, our learning objectives are:\n",
        "\n",
        "- Understand the structure of an HTML document and use that structure to extract desired information\n",
        "- Use Python data structures such as lists, dictionaries, and Pandas DataFrames to store and manipulate information\n",
        "- Identify some other (semi-)structured formats commonly used for storing and transferring data, such as [JSON](https://en.wikipedia.org/wiki/JSON) and [CSV](https://en.wikipedia.org/wiki/Comma-separated_values)\n",
        "- Practice using Python packages such as [Beautiful Soap](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) and Pandas, including how to navigate their documentation to find functionality."
      ],
      "metadata": {
        "id": "UqvRARKugJGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Table of Contents:\n",
        "* Prerequisites\n",
        "* Introduction to web scraping: [Selenuim](https://selenium-python.readthedocs.io/getting-started.html) vs [Beautiful Soap](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) package\n",
        " * Setting the environment\n",
        " * Parse the HTML with __BeautifulSoup__\n",
        "* Concatenate and merge two or more dataframes\n",
        "* ŸçSave data frame as __*.csv__ file\n"
      ],
      "metadata": {
        "id": "rZrI3b9igM09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "\n",
        "from urllib.request import Request, urlopen\n",
        "#import urllib\n",
        "import urllib.error\n",
        "\n",
        "\n",
        "import time\n",
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# Setting up 'requests' to make HTTPS requests properly takes some extra steps... we'll skip them for now.\n",
        "requests.packages.urllib3.disable_warnings()"
      ],
      "metadata": {
        "id": "Dc3cQHbNgeAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology\n",
        "Here‚Äôs step by step outline of this project:\n",
        "\n",
        "* Download the google scholar webpage of related topic using requests\n",
        "* Parse the HTML code source code using beautiful soup\n",
        "* Extract Title of the paper , Number of citation , Author of the paper , Year of Publication , Place of Publication from page\n",
        "* Compile the data and create a CSV file using pandas\n",
        "\n",
        "### Download the __Google scholar citation__ webpage using requests\n",
        "To begin , we‚Äôll use the requests Python library to download the web page. We can use ___requests.get___ to download a page . Here we also need to define __headers__ in this function because google scholar webpage required login.\n"
      ],
      "metadata": {
        "id": "ra3MCQ9Yg3sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import VimeoVideo\n",
        "VimeoVideo(\"733383823\", h=\"d6228d4de1\", width=600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "CYtGLPIbggYr",
        "outputId": "5fe661fd-1326-41a5-8877-a6a6c3bd7e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.VimeoVideo at 0x7f699c707940>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"300\"\n",
              "            src=\"https://player.vimeo.com/video/733383823?h=d6228d4de1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepared Data\n",
        "## Import\n",
        "\n",
        "In the previous lab, we got our data frame by exploring the orginazation page on google scholar citation page. Among data that we obtained was __link attribute__ that refers to author's google citation page.  "
      ],
      "metadata": {
        "id": "PmfKSvUviwvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VimeoVideo(\"656703362\", h=\"bae256298f\", width=600)"
      ],
      "metadata": {
        "id": "OjgAU1ppj5HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.1:** Write function named _get_link_to_profile( )_ that takes a path to data source ([Pandas Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)) that was genereted in __LAB 01 (Web Scraping)__ and returns a sliced pd.Series which contains active link to researchers' google citation profiles."
      ],
      "metadata": {
        "id": "azxzfyDcgY-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJT8MOOhhQz0"
      },
      "outputs": [],
      "source": [
        "def wrangle(file_path):\n",
        "  \"\"\"Creates CSV file with all authors' information.\n",
        "\n",
        "    CSV file name will include today's date, e.g. `'2023-03-28__authors_list.csv`'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name : suggested name\n",
        "        csv file name where you saved the data obtained at the previous lab.\n",
        "    Returns\n",
        "    -------\n",
        "    mask_lnk: pd.Series\n",
        "  \"\"\"\n",
        "  # Read CSV file into DataFrame\n",
        "  df = pd.read_csv(file_path)\n",
        "  # Subset to google citation link of researchers' profiles\n",
        "  mask_lnk = df [\"link\"]\n",
        "\n",
        "  return pd.Series(mask_lnk)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a function written, let's test it out."
      ],
      "metadata": {
        "id": "pLVwLeUJn_8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VimeoVideo(\"656701336\", h=\"c3a3e9bc16\", width=600)"
      ],
      "metadata": {
        "id": "iXp85Ej3hSkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2:** Use your `wrangle` function to create a DataFrame `df` from the CSV file `./content/2023-03-23_authors_list.csv`.\n",
        "__Attention:__\n",
        ">  **Note!‚ö†Ô∏è**:You have to pass the right name of your dataset. Note that, the name of *.csv has the following convension: **date in (%Y-%m-%d) format followed by ___authors_list.csv__."
      ],
      "metadata": {
        "id": "fmza6sT4oNNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = wrangle(\"/content/2023-03-23_authors_list.csv\")\n",
        "print(\"df shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "OIiAtK7gpMBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, your DataFrame `df` should have no more than 404 observations."
      ],
      "metadata": {
        "id": "b6sXKqq9rszY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your work\n",
        "assert (\n",
        "    len(df) <= 404\n",
        "), f\"`df` should have no more than 8606 observations, not {len(df)}.\""
      ],
      "metadata": {
        "id": "-RAwkFuUrtzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 2.3__: Set your user agent and assign it to avarible name _user_agent_. Then create a dictinary _headers_ and store your user agent."
      ],
      "metadata": {
        "id": "-KFEmmGNsPen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'\n",
        "headers = {'User-Agent': user_agent}"
      ],
      "metadata": {
        "id": "YwLEeLCosfq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 2.4:__ Write a function called ``` author_page()``` that get access to the researcher's profile on Google scholar citation and returs html page content and response status. As example, take the first record of the pd.Series that you got in the previous __Task 2.2__."
      ],
      "metadata": {
        "id": "0e3geXHpsgg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def author_page(lnk: pd.Series):\n",
        "\n",
        "  url = lnk[0]\n",
        "  status=0\n",
        "  try:\n",
        "      reqest = requests.get(url)\n",
        "  except urllib.error.HTTPError as e:\n",
        "      # Return code error (e.g. 404, 501, ...)\n",
        "      # ...\n",
        "      print('HTTPError: {}'.format(e.code))\n",
        "  except urllib.error.URLError as e:\n",
        "      # Not an HTTP-specific error (e.g. connection refused)\n",
        "      # ...\n",
        "      print('URLError: {}'.format(e.reason))\n",
        "  else:\n",
        "      status= 200\n",
        "\n",
        "  reqest = Request(url,headers=headers)\n",
        "  page = urlopen(reqest)\n",
        "  soup = BeautifulSoup(page, \"lxml\")\n",
        "\n",
        "  return status, soup\n",
        "\n",
        "status, src = author_page(df)\n",
        "print(f'The requested page is rendered successfully: the response status is good: {status}')\n",
        "print(src.prettify())"
      ],
      "metadata": {
        "id": "RfH6lW_Oti2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Issue in some Google Citation Pages üèÉ\n",
        "Maybe you note that at the bottom of some scholare pages the \"show more\" button presents. To load more articles, you have to click first on the button and the broweser will render and refresh the content again adding more articles and associated information to the page.\n",
        "To capture the whole html page, in this lab we provide an basic solution using <font color=\"green\">\"pagesize\" </font> parameter.\n",
        "\n"
      ],
      "metadata": {
        "id": "EqdLnSVyPn31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 2.5__: **Create** a dictionary _params_ that contains all necessary parameters that you have to pass in the url. Do not forget to add __pagesize__ to the _params_ dictionary.\n",
        "\n"
      ],
      "metadata": {
        "id": "C0Zne0pyXPtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "        \"hl\": \"en\",                                          # language\n",
        "        \"pagesize\": 20                                       # page size\n",
        "    }"
      ],
      "metadata": {
        "id": "IPUZS-ZUYwkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 2.6__: **Test** URL and pass __pagesize__ as parameter. Use the first record in your ```df```.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i_x9qy5DZxjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_link= df[0]\n",
        "full_url = page_link +'&pagesize='+ str(params['pagesize'])\n",
        "print(full_url)"
      ],
      "metadata": {
        "id": "18GXV4gtmQOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 2.7:__ Parse the html content provided by the url sorted in __`full_url`__ variable to:\n",
        "1. get the following information:\n",
        "  * _paper title_\n",
        "  * _co-authors_\n",
        "  * _cited by count_\n",
        "  * _journal name_\n",
        "  * and, _publication time_.\n",
        "\n",
        "2. save the obtained information in adictionary called __info__."
      ],
      "metadata": {
        "id": "VcJhv88R7_cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_url= 'https://scholar.google.com//citations?hl=en&user=Ao9Y41oAAAAJ&pagesize=20'\n",
        "reqest = Request(full_url,headers=headers)\n",
        "page = urlopen(reqest)\n",
        "soup = BeautifulSoup(page, \"lxml\")\n",
        "print(f\"extracting authors page size to #{params['pagesize']}.\")\n",
        "print(full_url)\n",
        "#print(f'The requested page is rendered successfully: the response status is good: {status}')\n",
        "#print(src.prettify())\n",
        "info=[]\n",
        "paper = soup.find_all('tr',{'class':'gsc_a_tr'})\n",
        "for td in paper:\n",
        "      info.append({\n",
        "          'paper_title': td.find(\"a\", {'class':'gsc_a_at'}).text,\n",
        "          'authors_list':td.find(\"div\", {'class':'gs_gray'}).text ,\n",
        "          'cited by': td.find('a',\\\n",
        "                              {'class':'gsc_a_ac gs_ibl'}).text if td.find('a',\\\n",
        "                                                                           {'class':'gsc_a_ac gs_ibl'})  else 0,\n",
        "          'journal_name': td.find_all(\"div\", {'class':'gs_gray'})[1].text ,\n",
        "          'publication_time':  re.search(r'(\\d\\d\\d\\d)',\n",
        "                                            str(td.find(\"span\",\\\n",
        "                                                {'class':'gsc_a_h gsc_a_hc gs_ibl'}))).group(1) if td.find(\"span\",\\\n",
        "                                                              {'class':'gsc_a_h gsc_a_hc gs_ibl'}).contents  else 'None',\n",
        "                   })\n",
        "\n",
        "author_df= pd.DataFrame(info)\n",
        "author_df"
      ],
      "metadata": {
        "id": "RaBqu1p580ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 2.8__: Now, since you knaw how to pass _page size_ paramater through URL, your task is to create a function called __scrape_first_100_records( )__ which takes __page_link__ of the author as a parameter and returns a data frame containg the full information listed in __Task 2.7__.\n",
        "\n",
        "__Attention:__\n",
        ">  **Note!‚ö†Ô∏è**: The current solution works only till the page size reaches 100. When there are many articles, you have to seek another solution. This is why we called the function such that."
      ],
      "metadata": {
        "id": "aVN_AdAemfPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_first_100_records(page_link: str)-> pd.DataFrame:\n",
        "    params = {\n",
        "        \"hl\": \"en\",                                          # language\n",
        "        \"pagesize\": 20                                       # page size\n",
        "    }\n",
        "    authors_lst = []\n",
        "    profiles_is_present = True\n",
        "    while profiles_is_present:\n",
        "        url = page_link +'&pagesize='+ str(params['pagesize'])\n",
        "\n",
        "        reqest = Request(url,headers=headers)\n",
        "        page = urlopen(reqest)\n",
        "        soup = BeautifulSoup(page, \"lxml\")\n",
        "        print(f\"extracting author page: {url}.\")\n",
        "\n",
        "        info=[]\n",
        "        paper = soup.find_all('tr',{'class':'gsc_a_tr'})\n",
        "        for td in paper:\n",
        "          info.append({\n",
        "                    'author_page': page_link,\n",
        "                    'paper_title': td.find(\"a\", {'class':'gsc_a_at'}).text,\n",
        "                    'authors_list':td.find(\"div\", {'class':'gs_gray'}).text ,\n",
        "                    'cited by': td.find('a', {'class':'gsc_a_ac gs_ibl'}).text if td.find('a',\n",
        "                                                                                          {'class':'gsc_a_ac gs_ibl'}) else 0,\n",
        "                    'journal_name': td.find_all(\"div\", {'class':'gs_gray'})[1].text ,\n",
        "                    'publication_time': re.search(r'(\\d\\d\\d\\d)',\n",
        "                                        str(td.find(\"span\",\\\n",
        "                                                    {'class':'gsc_a_h gsc_a_hc gs_ibl'}))).group(1) if td.find(\"span\",\\\n",
        "                                                                                                      {'class':\\\n",
        "                                                                                                       'gsc_a_h gsc_a_hc gs_ibl'\n",
        "                                                                                                       }).contents  else 'None',\n",
        "                   })\n",
        "        # if next page token is present -> update next page token and increment 10 to get the next page\n",
        "        # next_author = soup.findChildren(\"button\")\n",
        "        show_more = soup.findChildren(\"button\",{'id':'gsc_bpf_more', 'disabled':''})\n",
        "        extend= re.search(r\"disabled=\"\"\", str(show_more))\n",
        "        # print('********')\n",
        "        # print(show_more)\n",
        "\n",
        "        # print(extend)\n",
        "        # print('*******')\n",
        "        if ((not extend) and (params[\"pagesize\"] != 100)):\n",
        "            params[\"pagesize\"] += 20\n",
        "            time.sleep(3)\n",
        "        else:\n",
        "            profiles_is_present = False\n",
        "            reset={\"pagesize\": 20}\n",
        "            params.update(reset)\n",
        "            print('**********')\n",
        "\n",
        "\n",
        "        # print(authors)\n",
        "    return pd.DataFrame(info)\n",
        "df_auth_list= pd.DataFrame()\n",
        "for row in df:\n",
        "  updated= scrape_first_100_records(row)\n",
        "  df_auth_list= pd.concat([updated, df_auth_list])\n",
        "  print(df_auth_list.shape)\n",
        "\n",
        "df_auth_list"
      ],
      "metadata": {
        "id": "02761hhbPqoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 2.9:__ Check the size of _df_auth_list_ dataframe. _**What do you see?**_\n",
        "* Use [shape](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html) property of dataframe to explore the size of dataframe\n",
        "* Refere to the [TU home page](https://www.taibahu.edu.sa/Pages/AR/Home.aspx). Under [ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖŸÅÿ™Ÿàÿ≠ÿ©](https://www.taibahu.edu.sa/Pages/AR/CustomPage.aspx?ID=87) section, find the __updated excel file__ that contain the necessary information to count how many academic staff there are in total.\n",
        "* Create a variable called __portion_of_authors__ that referes to the ration of authors whose an account on google scholare citation."
      ],
      "metadata": {
        "id": "f4QAKoOvYG5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_auth_list.shape)\n",
        "portion_of_authors= df_auth_list.shape[0] /3720 # the number might change year by year.\n",
        "print(f'Ratio of scholars whose account on google scholare citation is {round(portion_of_authors,2)} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmjh_AYFYjVX",
        "outputId": "dcef6703-413c-4753-8f39-32691757dac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9068, 6)\n",
            "Ratio of scholars whose account on google scholare citation is 2.44 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 2.10:__ Count how many authors contributed to each paper. Create the new column called __#of_authors__ that contains such information. use [apply function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html)."
      ],
      "metadata": {
        "id": "rjxn_aKrrPVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def count_authors(df):\n",
        "  count= len(str(df).split(','))\n",
        "  return count\n",
        "\n",
        "df_auth_list['#of_authors'] = df_auth_list['authors_list'].apply(count_authors)\n",
        "df_auth_list\n"
      ],
      "metadata": {
        "id": "fNcSdtcxlYnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 2.11:__ It is time to save the collected data as _*.csv file_. Create a function called __save_as_csv()__ which takes as a parameter: __name of file__, __the directory__, and dataframe that you generated before."
      ],
      "metadata": {
        "id": "-3VADwS1e1Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_as_csv(file_name: str, directory= \".\"):\n",
        "  \"\"\"Creates CSV file with all authors' information.\n",
        "\n",
        "    CSV file name will include today's date, e.g. `'2023-03-28__authors_list.csv`'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name : suggested name\n",
        "        Observations with group assignment.\n",
        "    directory : str, default='.'\n",
        "        Location for saved CSV file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "  # Create filename with date\n",
        "  date_string = pd.Timestamp.now().strftime(format= \"%Y-%m-%d\")\n",
        "  final_filename = directory + \"/\" + date_string +'_' + file_name+ '.csv'\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "  df_auth_list.to_csv(final_filename)\n",
        "\n",
        "save_as_csv('auth_paper_lst')"
      ],
      "metadata": {
        "id": "l0OWaHE7e9rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Copyright 2023 Taibah University. This\n",
        "content is created, prepared by [Associate Prof. Dr. Mohammed Al-Sarem](https://sites.google.com/site/alsaremmh) and licensed solely for personal use. Redistribution or publication of this material is strictly prohibited."
      ],
      "metadata": {
        "id": "utxpd743fAzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv('/content/2023-03-23_authors_list.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "yVSvYa20w4Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['interst_new']= df['interst'].apply(lambda st: st[st.find('\">')+1:st.find(\"</a>\")])\n",
        "df['interst_new2']= df['interst_new'].apply(lambda st: st[st.find('\">')+1:st.find(\"</a>\")])"
      ],
      "metadata": {
        "id": "KB5euRDwxLWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('new.csv')"
      ],
      "metadata": {
        "id": "TQ0YM9w4yYA6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}